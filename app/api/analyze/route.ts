import { NextResponse } from "next/server";
import { GoogleGenerativeAI } from "@google/generative-ai";
import { db } from "@/lib/firebase"; // å¼•å…¥ä½ ä¹‹å‰çš„ Firebase å®ä¾‹
import { collection, getDocs } from "firebase/firestore";

// åˆå§‹åŒ– Gemini
const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY as string);

export async function POST(req: Request) {
  try {
    const { userInput, imageBase64 } = await req.json();

    if (imageBase64) {
      console.log("ğŸ“¸ æ¥æ”¶åˆ°å›¾ç‰‡æ•°æ®ï¼Œé•¿åº¦ä¸º:", imageBase64.length);
    }

    // 1. ä» Firestore è·å–æœ€æ–°çš„æ´åŠ©èµ„æºåº“æ•°æ®
    const resourcesCol = collection(db, "resources");
    const resourceSnapshot = await getDocs(resourcesCol);
    const database = resourceSnapshot.docs.map(doc => doc.data());

    // 2. é€‰æ‹© Gemini 1.5 Flash æ¨¡å‹ (é€Ÿåº¦æœ€å¿«ï¼Œé€‚åˆé»‘å®¢æ¾ Demo)
    const model = genAI.getGenerativeModel({ model: "gemini-2.5-flash" });

    // 3. ç¼–å†™ç»™ AI çš„ç³»ç»Ÿçº§æŒ‡ä»¤ (In-Context Learning)
    const prompt = `
      You are a professional Malaysian B40 aid matching assistant. 
      
      ã€MISSIONã€‘:
      1. If an image is provided, perform OCR to identify the institution (e.g., Hospital, School, TNB) and the issue (e.g., overdue bill, medical cost).
      2. **LANGUAGE ADAPTIVITY**: Detect the language used by the user in their text or the uploaded image (English, Malay, or Chinese). 
      3. Your "reason" field MUST be written in the SAME language the user used. If multiple languages are detected, default to English.

      ã€RESOURCE DATABASEã€‘:
      ${JSON.stringify(database)}

      User Input: "${userInput || "No text provided, please analyze the image"}"

      ã€OUTPUT FORMATã€‘:
      Return ONLY a JSON object. No markdown. No extra text.
      {
        "matches": [
          {
            "id": "resource_id",
            "name": "Name of the aid",
            "reason": "Explain WHY this matches in the user's detected language (e.g., if user asks in Chinese, answer in Chinese)",
            "confidence": 98,
            "application_url": "URL",
            "lat": 3.1412,
            "lng": 101.6865
          }
        ]
      }
    `;

    // æ ¸å¿ƒå¤šæ¨¡æ€é€»è¾‘ï¼šåˆ¤æ–­æ˜¯å¦æœ‰å›¾ç‰‡ï¼Œå¦‚æœæœ‰ï¼ŒæŒ‰æ ¼å¼è£…è½½
    let promptParts: any[] = [{ text: prompt }];

    if (imageBase64) {
      // å»é™¤å‰é¢çš„ "data:image/jpeg;base64," æ ‡å¤´ï¼Œåªä¿ç•™çº¯æ•°æ®
      const base64Data = imageBase64.split(",")[1]; 
      const mimeType = imageBase64.split(";")[0].split(":")[1] || "image/jpeg";
      
      promptParts.push({
        inlineData: {
          data: base64Data,
          mimeType: mimeType,
        },
      });
    }

    // 4. è¯·æ±‚ AI å¹¶åœ¨è®¾å®šä¸­å¼ºåˆ¶è¦æ±‚è¿”å› JSON
    const result = await model.generateContent({
      contents: [{ role: "user", parts: promptParts }],
      generationConfig: {
        responseMimeType: "application/json", // å¼ºåˆ¶è¾“å‡º JSON æ ¼å¼ï¼ŒHackathon å¿…å¤‡é˜²ç¿»è½¦æŠ€å·§ï¼
      },
    });

    let responseText = result.response.text();
    // è‡ªåŠ¨å‰¥ç¦»å¯èƒ½å¸¦æœ‰çš„ markdown æ ‡è®°ï¼Œé˜²æ­¢ JSON.parse æŠ¥é”™
    responseText = responseText.replace(/```json/g, "").replace(/```/g, "").trim();
    const aiData = JSON.parse(responseText);

    return NextResponse.json({ success: true, data: aiData });

  } catch (error) {
    console.error("AI åˆ†æå‡ºé”™:", error);
    return NextResponse.json({ success: false, error: "AI åˆ†æå¤±è´¥" }, { status: 500 });
  }
}